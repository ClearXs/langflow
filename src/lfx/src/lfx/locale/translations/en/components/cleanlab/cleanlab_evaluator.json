{
  "display_name": "Cleanlab Evaluator",
  "description": "Evaluates any LLM response using Cleanlab and outputs trust score and explanation.",
  "system_prompt": {
    "display_name": "System Message",
    "info": "System-level instructions prepended to the user query."
  },
  "prompt": {
    "display_name": "Prompt",
    "info": "The user's query to the model."
  },
  "response": {
    "display_name": "Response",
    "info": "The response to the user's query."
  },
  "api_key": {
    "display_name": "Cleanlab API Key",
    "info": "Your Cleanlab API key."
  },
  "model": {
    "display_name": "Cleanlab Evaluation Model",
    "info": "The model Cleanlab uses to evaluate the response. This does NOT need to be the same model that generated the response."
  },
  "quality_preset": {
    "display_name": "Quality Preset",
    "info": "This determines the accuracy, latency, and cost of the evaluation. Higher quality is generally slower but more accurate."
  },
  "outputs": {
    "response": {
      "display_name": "Response"
    },
    "score": {
      "display_name": "Trust Score"
    },
    "explanation": {
      "display_name": "Explanation"
    }
  },
  "status": {
    "score": "Trust score: {score:.2f}",
    "passing_response": "Passing through response."
  },
  "errors": {
    "no_explanation": "No explanation returned."
  }
}