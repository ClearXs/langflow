{
  "display_name": "Cleanlab RAG Evaluator",
  "description": "Evaluates context, query, and response from a RAG pipeline using Cleanlab and outputs trust metrics.",
  "api_key": {
    "display_name": "Cleanlab API Key",
    "info": "Your Cleanlab API key."
  },
  "model": {
    "display_name": "Cleanlab Evaluation Model",
    "info": "The model Cleanlab uses to evaluate the context, query, and response. This does NOT need to be the same model that generated the response."
  },
  "quality_preset": {
    "display_name": "Quality Preset",
    "info": "This determines the accuracy, latency, and cost of the evaluation. Higher quality is generally slower but more accurate."
  },
  "context": {
    "display_name": "Context",
    "info": "The context retrieved for the given query."
  },
  "query": {
    "display_name": "Query",
    "info": "The user's query."
  },
  "response": {
    "display_name": "Response",
    "info": "The response generated by the LLM."
  },
  "run_context_sufficiency": {
    "display_name": "Run Context Sufficiency",
    "info": "Evaluate whether the retrieved context contains information needed to answer the query."
  },
  "run_response_groundedness": {
    "display_name": "Run Response Groundedness",
    "info": "Evaluate whether the response is supported directly by the context."
  },
  "run_response_helpfulness": {
    "display_name": "Run Response Helpfulness",
    "info": "Evaluate whether the response effectively addresses the user's query."
  },
  "run_query_ease": {
    "display_name": "Run Query Ease",
    "info": "Evaluate whether the user query seems easy for an AI system to properly handle."
  },
  "outputs": {
    "response": {
      "display_name": "Response"
    },
    "trust_score": {
      "display_name": "Trust Score"
    },
    "explanation": {
      "display_name": "Explanation"
    },
    "other_evals": {
      "display_name": "Other Evals"
    },
    "summary": {
      "display_name": "Evaluation Summary"
    }
  },
  "status": {
    "configuring_evals": "Configuring selected evals...",
    "running_evals": "Running evals: {evals}",
    "passing_response": "Passing through response.",
    "trust_score": "Trust Score: {score:.3f}",
    "explanation_extracted": "Trust explanation extracted.",
    "other_evals_returned": "{count} other evals returned.",
    "summary_built": "Evaluation summary built."
  },
  "success": {
    "evaluation_complete": "Evaluation complete."
  },
  "errors": {
    "evaluation_failed": "Evaluation failed: {error}"
  },
  "logs": {
    "configuring_evals": "Configuring evaluation metrics",
    "selected_evals": "Selected evaluations: {evals}",
    "running_evals": "Running {count} evaluation(s)",
    "passing_response": "Passing through original response",
    "trust_score_retrieved": "Trust score retrieved: {score:.3f}",
    "explanation_extracted": "Trust explanation extracted from results",
    "other_evals_retrieved": "Retrieved {count} additional evaluation metric(s)",
    "summary_built": "Evaluation summary built successfully"
  },
  "summary": {
    "trustworthiness": "Trustworthiness: {score:.3f}",
    "explanation": "Explanation: {explanation}",
    "metric": "{name}: {score:.3f}",
    "full": "Query:\n{query}\n-----\nContext:\n{context}\n-----\nResponse:\n{response}\n------------------------------\n{metrics}"
  }
}